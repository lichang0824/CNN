{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585941ef-25c1-463d-b168-39ce2b474ce1",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6061aa55-251b-4364-b330-988f655c4917",
   "metadata": {
    "id": "6061aa55-251b-4364-b330-988f655c4917",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
   "metadata": {
    "id": "75409c2a-7c7e-454a-b493-a8894b7469ce",
    "outputId": "124f3a64-fd41-43f6-8142-e776ca7c728a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
   "metadata": {
    "id": "6923a256-f4d8-4be9-8cd5-4151db71895e",
    "outputId": "d4ecf0c7-c798-41ef-9d25-e184eaf4aaf7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchangli_824\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f42434-93ee-4923-9809-4aed63ddcce2",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
   "metadata": {
    "id": "8d2623ad-319a-4cb5-89cf-f279814a831e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "\n",
    "        self.up_sampling_2 = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        self.conv64_1_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_384_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 384, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_160_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 160, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_40_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 40, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 1, kernel_size = kernel_size, padding = 'same'),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv64_1_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        feature_map_64 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        feature_map_32 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        feature_map_16 = x.detach()\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_16, x), dim = 1)\n",
    "        x = self.conv16_384_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_32, x), dim = 1)\n",
    "        x = self.conv32_160_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.up_sampling_2(x)\n",
    "        x = torch.cat((feature_map_64, x), dim = 1)\n",
    "        x = self.conv64_40_8(x)\n",
    "        x = self.conv64_8_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d0082e-99dc-49ac-aa28-b190cfc76d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetScalarLabel(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, activation_fn = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pooling_2 = nn.MaxPool3d(kernel_size = 2)\n",
    "        self.max_pooling_1 = nn.MaxPool3d(kernel_size = 1)\n",
    "\n",
    "        self.conv256_1_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 1, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv256_2_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 2, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 2),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_2_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 2, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv128_4_4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 4, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 4),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_4_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 4, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv64_8_8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 8),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_8_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv32_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_32_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv16_128_128 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 128, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 128),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_128_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 128, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv8_256_256 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 256, kernel_size = kernel_size, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 256),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_256_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 256, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv4_32_32 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = 3, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 32),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        self.conv2_32_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 32, out_channels = 1, kernel_size = 1, padding = 'same'),\n",
    "            nn.BatchNorm3d(num_features = 1),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv256_1_2(x)\n",
    "        x = self.conv256_2_2(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv128_2_4(x)\n",
    "        x = self.conv128_4_4(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv64_4_8(x)\n",
    "        x = self.conv64_8_8(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv32_8_32(x)\n",
    "        x = self.conv32_32_32(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv16_32_128(x)\n",
    "        x = self.conv16_128_128(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv8_128_256(x)\n",
    "        x = self.conv8_256_256(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv4_256_32(x)\n",
    "        x = self.conv4_32_32(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        x = self.conv2_32_1(x)\n",
    "        x = self.max_pooling_2(x)\n",
    "        return torch.reshape(x, (x.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676a283b-9009-4ac8-8fd3-c1ef8b65c4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, training_loader, optimizer, loss_fn):\n",
    "    cumulative_loss = 0.0\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        wandb.log({'batch loss': loss.item()})\n",
    "    return cumulative_loss / len(training_loader), cumulative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
   "metadata": {
    "id": "af6093e9-59db-4985-91a0-cf5d2bb9b3e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config, loss_fn):\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # initialize a wandb run\n",
    "    wandb.init(config = config)\n",
    "\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    print('config:', config)\n",
    "\n",
    "    # get training loader\n",
    "    training_loader = DataLoader(list(zip(X_train, y_train)), batch_size = config.batch_size, shuffle = True)\n",
    "\n",
    "    # initialize model\n",
    "    if config.activation_fn == 'ReLU':\n",
    "        activation_fn = nn.ReLU()\n",
    "    \n",
    "    if config.activation_fn == 'Sigmoid':\n",
    "        activation_fn = nn.Sigmoid()\n",
    "    \n",
    "    model = ConvNetScalarLabel(kernel_size = config.kernel_size, activation_fn = activation_fn).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = config.learning_rate, momentum = 0.9)\n",
    "\n",
    "    for epoch in range(config.epochs_choice):\n",
    "        avg_loss_per_batch, cumulative_loss = train_epoch(model, training_loader, optimizer, loss_fn)\n",
    "        wandb.log({'avg_loss_per_batch': avg_loss_per_batch, 'cumulative_loss': cumulative_loss})\n",
    "        print(f'Loss for epoch {epoch}: {cumulative_loss}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77c5334-2ce1-49ea-8785-c499da625081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model, loss_fn):\n",
    "    # copy the config\n",
    "    config = wandb.config\n",
    "    \n",
    "    # get testing loader\n",
    "    testing_loader = DataLoader(list(zip(X_test, y_test)), batch_size = config.batch_size, shuffle = True)\n",
    "    \n",
    "    testing_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, data in enumerate(testing_loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        testing_loss += loss.item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy().tolist())\n",
    "        y_pred.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    y_true = list(itertools.chain.from_iterable(y_true))\n",
    "    y_pred = list(itertools.chain.from_iterable(y_pred))\n",
    "    return testing_loss / len(testing_loader), testing_loss, r2_score(y_true = y_true, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45e5a4d-fe1f-4629-a456-1865f7e9906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config = None):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = train(config, loss_fn)\n",
    "    avg_loss_per_batch_test, testing_loss, r2 = test(config, model, loss_fn)\n",
    "    wandb.log({'avg_loss_per_batch_test': avg_loss_per_batch_test, 'testing_loss': testing_loss, 'r2': r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46330721-d5b7-416a-86fa-4f3b16d31e54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Binvox Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f84b18-6c7d-45e9-baa3-9bf5ea3b5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Binvox Read Write\n",
    "#  Copyright (C) 2012 Daniel Maturana\n",
    "#  This file is part of binvox-rw-py.\n",
    "#\n",
    "#  binvox-rw-py is free software: you can redistribute it and/or modify\n",
    "#  it under the terms of the GNU General Public License as published by\n",
    "#  the Free Software Foundation, either version 3 of the License, or\n",
    "#  (at your option) any later version.\n",
    "#\n",
    "#  binvox-rw-py is distributed in the hope that it will be useful,\n",
    "#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#  GNU General Public License for more details.\n",
    "#\n",
    "#  You should have received a copy of the GNU General Public License\n",
    "#  along with binvox-rw-py. If not, see <http://www.gnu.org/licenses/>.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Voxels(object):\n",
    "    \"\"\" Holds a binvox model.\n",
    "    data is either a three-dimensional numpy boolean array (dense representation)\n",
    "    or a two-dimensional numpy float array (coordinate representation).\n",
    "\n",
    "    dims, translate and scale are the model metadata.\n",
    "\n",
    "    dims are the voxel dimensions, e.g. [32, 32, 32] for a 32x32x32 model.\n",
    "\n",
    "    scale and translate relate the voxels to the original model coordinates.\n",
    "\n",
    "    To translate voxel coordinates i, j, k to original coordinates x, y, z:\n",
    "\n",
    "    x_n = (i+.5)/dims[0]\n",
    "    y_n = (j+.5)/dims[1]\n",
    "    z_n = (k+.5)/dims[2]\n",
    "    x = scale*x_n + translate[0]\n",
    "    y = scale*y_n + translate[1]\n",
    "    z = scale*z_n + translate[2]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, dims, translate, scale, axis_order):\n",
    "        self.data = data\n",
    "        self.dims = dims\n",
    "        self.translate = translate\n",
    "        self.scale = scale\n",
    "        assert (axis_order in ('xzy', 'xyz'))\n",
    "        self.axis_order = axis_order\n",
    "\n",
    "    def clone(self):\n",
    "        data = self.data.copy()\n",
    "        dims = self.dims[:]\n",
    "        translate = self.translate[:]\n",
    "        return Voxels(data, dims, translate, self.scale, self.axis_order)\n",
    "\n",
    "    def write(self, fp):\n",
    "        write(self, fp)\n",
    "\n",
    "def read_header(fp):\n",
    "    \"\"\" Read binvox header. Mostly meant for internal use.\n",
    "    \"\"\"\n",
    "    line = fp.readline().strip()\n",
    "    if not line.startswith(b'#binvox'):\n",
    "        raise IOError('Not a binvox file')\n",
    "    dims = list(map(int, fp.readline().strip().split(b' ')[1:]))\n",
    "    translate = list(map(float, fp.readline().strip().split(b' ')[1:]))\n",
    "    scale = list(map(float, fp.readline().strip().split(b' ')[1:]))[0]\n",
    "    line = fp.readline()\n",
    "    return dims, translate, scale\n",
    "\n",
    "def read_as_3d_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as array.\n",
    "\n",
    "    Returns the model with accompanying metadata.\n",
    "\n",
    "    Voxels are stored in a three-dimensional numpy array, which is simple and\n",
    "    direct, but may use a lot of memory for large models. (Storage requirements\n",
    "    are 8*(d^3) bytes, where d is the dimensions of the binvox model. Numpy\n",
    "    boolean arrays use a byte per element).\n",
    "\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "    # if just using reshape() on the raw data:\n",
    "    # indexing the array as array[i,j,k], the indices map into the\n",
    "    # coords as:\n",
    "    # i -> x\n",
    "    # j -> z\n",
    "    # k -> y\n",
    "    # if fix_coords is true, then data is rearranged so that\n",
    "    # mapping is\n",
    "    # i -> x\n",
    "    # j -> y\n",
    "    # k -> z\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "    data = np.repeat(values, counts).astype(bool)\n",
    "    data = data.reshape(dims)\n",
    "    if fix_coords:\n",
    "        # xzy to xyz TODO the right thing\n",
    "        data = np.transpose(data, (0, 2, 1))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        axis_order = 'xzy'\n",
    "    return Voxels(data, dims, translate, scale, axis_order)\n",
    "\n",
    "def read_as_coord_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as coordinates.\n",
    "\n",
    "    Returns binvox model with voxels in a \"coordinate\" representation, i.e.  an\n",
    "    3 x N array where N is the number of nonzero voxels. Each column\n",
    "    corresponds to a nonzero voxel and the 3 rows are the (x, z, y) coordinates\n",
    "    of the voxel.  (The odd ordering is due to the way binvox format lays out\n",
    "    data).  Note that coordinates refer to the binvox voxels, without any\n",
    "    scaling or translation.\n",
    "\n",
    "    Use this to save memory if your model is very sparse (mostly empty).\n",
    "\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "\n",
    "    sz = np.prod(dims)\n",
    "    index, end_index = 0, 0\n",
    "    end_indices = np.cumsum(counts)\n",
    "    indices = np.concatenate(([0], end_indices[:-1])).astype(end_indices.dtype)\n",
    "\n",
    "    values = values.astype(np.bool)\n",
    "    indices = indices[values]\n",
    "    end_indices = end_indices[values]\n",
    "\n",
    "    nz_voxels = []\n",
    "    for index, end_index in zip(indices, end_indices):\n",
    "        nz_voxels.extend(range(index, end_index))\n",
    "    nz_voxels = np.array(nz_voxels)\n",
    "    # TODO are these dims correct?\n",
    "    # according to docs,\n",
    "    # index = x * wxh + z * width + y; // wxh = width * height = d * d\n",
    "\n",
    "    x = nz_voxels / (dims[0]*dims[1])\n",
    "    zwpy = nz_voxels % (dims[0]*dims[1]) # z*w + y\n",
    "    z = zwpy / dims[0]\n",
    "    y = zwpy % dims[0]\n",
    "    if fix_coords:\n",
    "        data = np.vstack((x, y, z))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        data = np.vstack((x, z, y))\n",
    "        axis_order = 'xzy'\n",
    "\n",
    "    #return Voxels(data, dims, translate, scale, axis_order)\n",
    "    return Voxels(np.ascontiguousarray(data), dims, translate, scale, axis_order)\n",
    "\n",
    "def dense_to_sparse(voxel_data, dtype=int):\n",
    "    \"\"\" From dense representation to sparse (coordinate) representation.\n",
    "    No coordinate reordering.\n",
    "    \"\"\"\n",
    "    if voxel_data.ndim!=3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3D array.')\n",
    "    return np.asarray(np.nonzero(voxel_data), dtype)\n",
    "\n",
    "def sparse_to_dense(voxel_data, dims, dtype=bool):\n",
    "    if voxel_data.ndim!=2 or voxel_data.shape[0]!=3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3xN array.')\n",
    "    if np.isscalar(dims):\n",
    "        dims = [dims]*3\n",
    "    dims = np.atleast_2d(dims).T\n",
    "    # truncate to integers\n",
    "    xyz = voxel_data.astype(np.int)\n",
    "    # discard voxels that fall outside dims\n",
    "    valid_ix = ~np.any((xyz < 0) | (xyz >= dims), 0)\n",
    "    xyz = xyz[:,valid_ix]\n",
    "    out = np.zeros(dims.flatten(), dtype=dtype)\n",
    "    out[tuple(xyz)] = True\n",
    "    return out\n",
    "\n",
    "#def get_linear_index(x, y, z, dims):\n",
    "    #\"\"\" Assuming xzy order. (y increasing fastest.\n",
    "    #TODO ensure this is right when dims are not all same\n",
    "    #\"\"\"\n",
    "    #return x*(dims[1]*dims[2]) + z*dims[1] + y\n",
    "\n",
    "def write(voxel_model, fp):\n",
    "    \"\"\" Write binary binvox format.\n",
    "\n",
    "    Note that when saving a model in sparse (coordinate) format, it is first\n",
    "    converted to dense format.\n",
    "\n",
    "    Doesn't check if the model is 'sane'.\n",
    "\n",
    "    \"\"\"\n",
    "    if voxel_model.data.ndim==2:\n",
    "        # TODO avoid conversion to dense\n",
    "        dense_voxel_data = sparse_to_dense(voxel_model.data, voxel_model.dims)\n",
    "    else:\n",
    "        dense_voxel_data = voxel_model.data\n",
    "\n",
    "    fp.write('#binvox 1\\n')\n",
    "    fp.write('dim '+' '.join(map(str, voxel_model.dims))+'\\n')\n",
    "    fp.write('translate '+' '.join(map(str, voxel_model.translate))+'\\n')\n",
    "    fp.write('scale '+str(voxel_model.scale)+'\\n')\n",
    "    fp.write('data\\n')\n",
    "    if not voxel_model.axis_order in ('xzy', 'xyz'):\n",
    "        raise ValueError('Unsupported voxel model axis order')\n",
    "\n",
    "    if voxel_model.axis_order=='xzy':\n",
    "        voxels_flat = dense_voxel_data.flatten()\n",
    "    elif voxel_model.axis_order=='xyz':\n",
    "        voxels_flat = np.transpose(dense_voxel_data, (0, 2, 1)).flatten()\n",
    "\n",
    "    # keep a sort of state machine for writing run length encoding\n",
    "    state = voxels_flat[0]\n",
    "    ctr = 0\n",
    "    for c in voxels_flat:\n",
    "        if c==state:\n",
    "            ctr += 1\n",
    "            # if ctr hits max, dump\n",
    "            if ctr==255:\n",
    "                fp.write(chr(state))\n",
    "                fp.write(chr(ctr))\n",
    "                ctr = 0\n",
    "        else:\n",
    "            # if switch state, dump\n",
    "            fp.write(chr(state))\n",
    "            fp.write(chr(ctr))\n",
    "            state = c\n",
    "            ctr = 1\n",
    "    # flush out remainders\n",
    "    if ctr > 0:\n",
    "        fp.write(chr(state))\n",
    "        fp.write(chr(ctr))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import doctest\n",
    "    doctest.testmod()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca78b2-c4d7-42d8-9892-f9208ea51be7",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26bbed78-d8aa-40c6-833b-5d3dbe0b264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../AdditiveParts/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36da023-a1cb-417a-a1a2-8b3cee3068cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(datapath + 'Tweaker Orientation Score/Tweaker Orientation Score/parts0_1-3950.csv', header = None)\n",
    "printability = pd.Series(data = csv[1].values, index = csv[0].values, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db8197f-05f3-4534-ad39-173917e16657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00007eb3-6ed3-45a9-8a41-990953ca0284.stl            6.602921\n",
       "00007eb3-6ed3-45a9-8a41-990953ca0284.stl-100.stl    2.559219\n",
       "00007eb3-6ed3-45a9-8a41-990953ca0284.stl0-10.stl    2.559219\n",
       "00007eb3-6ed3-45a9-8a41-990953ca0284.stl00-1.stl    6.602921\n",
       "00007eb3-6ed3-45a9-8a41-990953ca0284.stl010.stl     0.021325\n",
       "                                                      ...   \n",
       "6866d335-8aa6-47ba-9431-8bd95d7be4a2.stl-100.stl    2.226756\n",
       "6866d335-8aa6-47ba-9431-8bd95d7be4a2.stl0-10.stl    1.954118\n",
       "6866d335-8aa6-47ba-9431-8bd95d7be4a2.stl00-1.stl    2.559219\n",
       "6866d335-8aa6-47ba-9431-8bd95d7be4a2.stl010.stl     2.559219\n",
       "6866d335-8aa6-47ba-9431-8bd95d7be4a2.stl100.stl     1.988942\n",
       "Length: 22722, dtype: float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a456531c-12f6-4c5c-b048-cbe1d71a0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "count = 0\n",
    "directory = os.fsencode(datapath + 'parts_0, files 1 through 3950/Binvox_files_default_res/')\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    voxels = read_as_3d_array(open(datapath + 'parts_0, files 1 through 3950/Binvox_files_default_res/' + filename, 'rb')).data\n",
    "    inputs.append([voxels])\n",
    "    score = printability[filename.replace('binvox', 'stl')]\n",
    "    labels.append([score])\n",
    "    count += 1\n",
    "    if count >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7df8d2c-c9ba-4ffd-8354-6dd5c70949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.array(inputs), dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f20e46-e26b-49d4-922d-45233d012011",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(np.array(labels), dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ff9159-6663-4950-948f-375de80052aa",
   "metadata": {
    "id": "79ff9159-6663-4950-948f-375de80052aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = torch.rand((16, 1, 256, 256, 256), device = device)\n",
    "# y = torch.rand((16, 1, 1, 1, 1), device = device)\n",
    "X_train, X_test = torch.utils.data.random_split(X, [0.8, 0.2])\n",
    "y_train, y_test = torch.utils.data.random_split(y, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3855e-1af6-4dac-a2eb-9afca254bdcd",
   "metadata": {},
   "source": [
    "# Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2403886e-355f-4658-8199-aebb835516bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "metric = {\n",
    "    'name': 'testing_loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3, 4, 5]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU', 'Sigmoid']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5, 10, 20]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [1]\n",
    "    },\n",
    "}\n",
    "'''\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "        'values': [3]\n",
    "    },\n",
    "    'activation_fn': {\n",
    "        'values': ['ReLU']\n",
    "    },\n",
    "    'epochs_choice': {\n",
    "          'values': [5]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'values': [1e-4]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [1]\n",
    "    },\n",
    "}\n",
    "'''\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83411fe0-67e5-49c3-b22a-9d5ac1f5abd1",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74f5fe4f-57a0-4445-aaec-58183fb12613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 3ulhuxwx\n",
      "Sweep URL: https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/3ulhuxwx\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project = 'CNN_sweep_scalar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec8f8da-e378-4181-8bcc-fc83a0a26754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\silly bb\\Desktop\\Capstone\\CNN\\wandb\\run-20240121_055341-157xh3eo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/157xh3eo' target=\"_blank\">fearless-sweep-54</a></strong> to <a href='https://wandb.ai/changli_824/CNN_sweep_scalar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/3ulhuxwx' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/3ulhuxwx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/3ulhuxwx' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/sweeps/3ulhuxwx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/157xh3eo' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/runs/157xh3eo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'activation_fn': 'Sigmoid', 'batch_size': 1, 'epochs_choice': 20, 'kernel_size': 5, 'learning_rate': 0.01}\n",
      "Loss for epoch 0: 141.99145918390423\n",
      "Loss for epoch 1: 138.45127855565806\n",
      "Loss for epoch 2: 138.2393172190059\n",
      "Loss for epoch 3: 138.12936350021482\n",
      "Loss for epoch 4: 138.0694907785437\n",
      "Loss for epoch 5: 138.02959744929467\n",
      "Loss for epoch 6: 138.00078228640632\n",
      "Loss for epoch 7: 137.9795496001425\n",
      "Loss for epoch 8: 137.9624880021329\n",
      "Loss for epoch 9: 137.94938213438218\n",
      "Loss for epoch 10: 137.93830069421165\n",
      "Loss for epoch 11: 137.92926032796458\n",
      "Loss for epoch 12: 137.9214886004156\n",
      "Loss for epoch 13: 137.9148448456108\n",
      "Loss for epoch 14: 137.90905051232403\n",
      "Loss for epoch 15: 137.90401084374753\n",
      "Loss for epoch 16: 137.89958747921628\n",
      "Loss for epoch 17: 137.89555084964013\n",
      "Loss for epoch 18: 137.8920126991652\n",
      "Loss for epoch 19: 137.88883450305366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss_per_batch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_loss_per_batch_test</td><td>▁</td></tr><tr><td>batch loss</td><td>▁▁▁▁▁▂▁▂█▁▁▂▁▂▂▁▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂█▁</td></tr><tr><td>cumulative_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>r2</td><td>▁</td></tr><tr><td>testing_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss_per_batch</td><td>1.72361</td></tr><tr><td>avg_loss_per_batch_test</td><td>3.98093</td></tr><tr><td>batch loss</td><td>0.03235</td></tr><tr><td>cumulative_loss</td><td>137.88883</td></tr><tr><td>r2</td><td>-0.82135</td></tr><tr><td>testing_loss</td><td>79.61862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-54</strong> at: <a href='https://wandb.ai/changli_824/CNN_sweep_scalar/runs/157xh3eo' target=\"_blank\">https://wandb.ai/changli_824/CNN_sweep_scalar/runs/157xh3eo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240121_055341-157xh3eo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id = sweep_id, function = evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d71f0e-e630-4491-85ad-6182de3cd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
